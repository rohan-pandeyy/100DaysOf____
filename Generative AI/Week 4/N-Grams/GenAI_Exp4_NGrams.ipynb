{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram model\n",
        "\n",
        "## Part 1: Data Preparation & N-gram Implementation\n",
        "### Imports and Dataset Setup\n",
        "We will manually load the corpus provided in given PDF and prepare it for tokenization."
      ],
      "metadata": {
        "id": "SSXpz5GOWLoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f1v9T81V7Mk",
        "outputId": "038e0c43-8aa8-46ad-e625-dffdbf29496e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data (required for Colab)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Dataset\n",
        "# Text extracted from GenAI-Lab-Week4.pdf (Page 2)\n",
        "corpus_text = \"\"\"\n",
        "Artificial intelligence is transforming modern society.\n",
        "It is used in healthcare finance education and transportation.\n",
        "Machine learning allows systems to improve automatically with experience.\n",
        "Data plays a critical role in training intelligent systems.\n",
        "Large datasets help models learn complex patterns.\n",
        "Deep learning uses multi layer neural networks.\n",
        "Neural networks are inspired by biological neurons.\n",
        "Each neuron processes input and produces an output.\n",
        "Training a neural network requires optimization techniques.\n",
        "Gradient descent minimizes the loss function.\n",
        "Natural language processing helps computers understand human language.\n",
        "Text generation is a key task in nlp.\n",
        "Language models predict the next word or character.\n",
        "Recurrent neural networks handle sequential data.\n",
        "LSTM and GRU models address long term dependency problems.\n",
        "However rnn based models are slow for long sequences.\n",
        "Transformer models changed the field of nlp.\n",
        "They rely on self attention mechanisms.\n",
        "Attention allows the model to focus on relevant context.\n",
        "Transformers process data in parallel.\n",
        "This makes training faster and more efficient.\n",
        "Modern language models are based on transformers.\n",
        "Education is being improved using artificial intelligence.\n",
        "Intelligent tutoring systems personalize learning.\n",
        "Automated grading saves time for teachers.\n",
        "Online education platforms use recommendation systems.\n",
        "Technology enhances the quality of learning experiences.\n",
        "Ethical considerations are important in artificial intelligence.\n",
        "Fairness transparency and accountability must be ensured.\n",
        "AI systems should be designed responsibly.\n",
        "Data privacy and security are major concerns.\n",
        "Researchers continue to improve ai safety.\n",
        "Text generation models can create stories poems and articles.\n",
        "They are used in chatbots virtual assistants and content creation.\n",
        "Generated text should be meaningful and coherent.\n",
        "Evaluation of text generation is challenging.\n",
        "Human judgement is often required.\n",
        "Continuous learning is essential in the field of ai.\n",
        "Research and innovation drive technological progress.\n",
        "Students should build strong foundations in mathematics.\n",
        "Programming skills are important for ai engineers.\n",
        "Practical experimentation enhances understanding.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Dataset loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btlsqCWMWFmZ",
        "outputId": "306f3ce4-b3f1-4549-84d5-c18f3a9258f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "We will tokenize the text and convert it to lowercase to standardize the input."
      ],
      "metadata": {
        "id": "dZqXXALGXXlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Process the corpus\n",
        "tokens = preprocess_text(corpus_text)\n",
        "\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "print(f\"Sample tokens: {tokens[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdaiXDcNW-us",
        "outputId": "c634f06f-fd4a-4afe-bca7-b6432c79d09d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 340\n",
            "Sample tokens: ['artificial', 'intelligence', 'is', 'transforming', 'modern', 'society', '.', 'it', 'is', 'used']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the N-gram Model\n",
        "This section implements the logic to count N-grams and calculate their probabilities. This mirrors the logic in your ngram_notebook.ipynb but adds a randomized sampling method (generate_text) which is usually better for generation than pure greedy sampling (which tends to get stuck in loops)."
      ],
      "metadata": {
        "id": "FLG3CXKxWZ8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram Model Functions\n",
        "\n",
        "def count_ngrams(tokens, n):\n",
        "    \"\"\"Counts n-grams in the token list.\"\"\"\n",
        "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "    return Counter(ngrams)\n",
        "\n",
        "def train_ngram_model(tokens, n):\n",
        "    \"\"\"\n",
        "    Builds a probabilistic N-gram model.\n",
        "    Returns a nested dictionary: { context_tuple: { next_word: probability } }\n",
        "    \"\"\"\n",
        "    ngrams = count_ngrams(tokens, n)\n",
        "    context_counts = defaultdict(Counter)\n",
        "\n",
        "    # Count frequencies of next_words given a context\n",
        "    for ngram, count in ngrams.items():\n",
        "        context = ngram[:-1]\n",
        "        target = ngram[-1]\n",
        "        context_counts[context][target] += count\n",
        "\n",
        "    # Calculate probabilities\n",
        "    model = defaultdict(dict)\n",
        "    for context, next_words in context_counts.items():\n",
        "        total_count = sum(next_words.values())\n",
        "        for word, count in next_words.items():\n",
        "            model[context][word] = count / total_count\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(model, n, seed_text, max_length=50):\n",
        "    \"\"\"\n",
        "    Generates text using the trained model via weighted random sampling.\n",
        "    \"\"\"\n",
        "    # Preprocess seed\n",
        "    seed_tokens = preprocess_text(seed_text)\n",
        "\n",
        "    # Ensure seed is long enough; if not, pad or just use what we have (simple version)\n",
        "    if len(seed_tokens) < n-1:\n",
        "        print(f\"Error: Seed text must have at least {n-1} words.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Initialize context\n",
        "    current_context = tuple(seed_tokens[-(n-1):])\n",
        "    result = list(current_context)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Check if context exists in model\n",
        "        if current_context not in model:\n",
        "            break\n",
        "\n",
        "        # Get possible next words and their probabilities\n",
        "        possible_words = list(model[current_context].keys())\n",
        "        probabilities = list(model[current_context].values())\n",
        "\n",
        "        # Sample the next word based on probability\n",
        "        next_word = random.choices(possible_words, weights=probabilities, k=1)[0]\n",
        "\n",
        "        # Append to result\n",
        "        result.append(next_word)\n",
        "\n",
        "        # Update context (slide window)\n",
        "        current_context = tuple(result[-(n-1):])\n",
        "\n",
        "        # Stop if we generate a period (optional, makes output cleaner)\n",
        "        if next_word == '.':\n",
        "            break\n",
        "\n",
        "    return \" \".join(result)"
      ],
      "metadata": {
        "id": "jFtekgsnWHYg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Generate\n",
        "Now we train a Trigram model ($N=3$) and generate text. You can change N to 2 for a Bigram model if the output is too repetitive."
      ],
      "metadata": {
        "id": "qbFk4PM2XLYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execution\n",
        "\n",
        "# Parameters\n",
        "N = 3  # Trigram model (looks at previous 2 words to predict the 3rd)\n",
        "\n",
        "# Train the model\n",
        "ngram_model = train_ngram_model(tokens, N)\n",
        "print(f\"Model trained with N={N}. Vocabulary size: {len(set(tokens))}\")\n",
        "\n",
        "# --- Generation Examples ---\n",
        "\n",
        "# Example 1: Seed \"artificial intelligence\"\n",
        "seed1 = \"artificial intelligence\"\n",
        "output1 = generate_text(ngram_model, N, seed1)\n",
        "print(f\"\\nInput Seed: '{seed1}'\")\n",
        "print(f\"Generated: {output1}\")\n",
        "\n",
        "# Example 2: Seed \"neural networks\"\n",
        "seed2 = \"neural networks\"\n",
        "output2 = generate_text(ngram_model, N, seed2)\n",
        "print(f\"\\nInput Seed: '{seed2}'\")\n",
        "print(f\"Generated: {output2}\")\n",
        "\n",
        "# Example 3: Seed \"deep learning\"\n",
        "seed3 = \"deep learning\"\n",
        "output3 = generate_text(ngram_model, N, seed3)\n",
        "print(f\"\\nInput Seed: '{seed3}'\")\n",
        "print(f\"Generated: {output3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXmit1hFWI96",
        "outputId": "a7e033c1-03af-41fa-fa9c-21baa9c8e75c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained with N=3. Vocabulary size: 195\n",
            "\n",
            "Input Seed: 'artificial intelligence'\n",
            "Generated: artificial intelligence is transforming modern society .\n",
            "\n",
            "Input Seed: 'neural networks'\n",
            "Generated: neural networks .\n",
            "\n",
            "Input Seed: 'deep learning'\n",
            "Generated: deep learning uses multi layer neural networks are inspired by biological neurons .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdJiUi37Wlim"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}