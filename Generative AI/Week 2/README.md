# GenAI Experiment 2: GAN on MNIST

## Overview

This experiment implements a **Generative Adversarial Network (GAN)** using PyTorch to generate realistic handwritten digits based on the MNIST dataset.

The project consists of two competing neural networks:

1.  **Generator (G)**: Creates fake images from random noise.
2.  **Discriminator (D)**: Distinguishes between real MNIST images and fake images generated by G.

Over time, the Generator learns to produce increasingly realistic images to fool the Discriminator.

## Technical Details

### Dataset

- **Source**: MNIST (Modified National Institute of Standards and Technology) database.
- **Content**: 60,000 training images of handwritten digits (0-9).
- **Preprocessing**: Images are normalized to the range `[-1, 1]` to match the Tanh output of the Generator.

### Architecture

The experiment uses fully connected (dense) networks for both models.

#### Generator

- **Input**: Random noise vector of size 100.
- **Layers**:
    - Linear: 100 $\to$ 256 (LeakyReLU 0.2)
    - Linear: 256 $\to$ 512 (LeakyReLU 0.2)
    - Linear: 512 $\to$ 1024 (LeakyReLU 0.2)
    - Linear: 1024 $\to$ 784 (28x28 flattened image)
- **Output Activation**: `Tanh` (forces pixel values to be between -1 and 1).

#### Discriminator

- **Input**: Flattened image vectors (size 784).
- **Layers**:
    - Linear: 784 $\to$ 512 (LeakyReLU 0.2)
    - Linear: 512 $\to$ 256 (LeakyReLU 0.2)
    - Linear: 256 $\to$ 1 (Scalar probability)
- **Output Activation**: `Sigmoid` (outputs probability of input being real).

### Hyperparameters

- **Batch Size**: 128
- **Learning Rate**: 0.0002
- **Epochs**: 50
- **Noise Dimension**: 100
- **Optimizer**: Adam (for both G and D) with betas `(0.5, 0.999)`.
- **Device**: Automatically selects CUDA (GPU) if available, otherwise CPU.

### Training Stability Techniques

- **Label Smoothing**: Real images are assigned a label of **0.9** instead of 1.0. This prevents the Discriminator from becoming too confident too quickly, which allows the Generator to learn more effectively.

## Training Process

The training loop executes for 50 epochs. In each step:

1.  **Train Discriminator**:
    - Calculate loss on a batch of real images (Target: 0.9).
    - Calculate loss on a batch of fake images generated by G (Target: 0.0).
    - Backpropagate to update D's weights.

2.  **Train Generator**:
    - Generate fake images.
    - Feed them to D.
    - Calculate loss based on D's predictions (Target: 1.0). The Generator wants D to believe the fake images are real.
    - Backpropagate to update G's weights.

## Usage

Run the notebook `GenAI_Exp2.ipynb` to start training.

```bash
jupyter notebook GenAI_Exp2.ipynb
```

## Results

- **Checkpoints**: Generated images are saved every 5 epochs in the `results/` directory (e.g., `results/epoch_5.png`).
- **Final Output**: A grid of generated digits is saved as `results/final_grid.png` after training completes.
