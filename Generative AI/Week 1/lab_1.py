# -*- coding: utf-8 -*-
"""GenAi_lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1USOuh5QFFanjfofti77jh2Lhms8hGEAG

# **CSET419- Introduction to Generative AI**

Lab – 1

Implement a Simple Generative Algorithm for Data Generation

1.Select any domain for data generation (example: image data generation).
"""

!pip install -q diffusers transformers accelerate safetensors

from huggingface_hub import login
login()

"""2.Choose any pre-trained generative model (example: Stable Diffusion / any diffusion model)."""

import torch
from diffusers import StableDiffusionPipeline

model_id = "runwayml/stable-diffusion-v1-5"

pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16
).to("cuda")

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split

dog_breeds = [
    "Bernese Mountain Dog",
    "Saint Bernard",
    "Newfoundland",
    "Cane Corso",
    "Greyhound",
    "Whippet","Chow Chow",
    "Shiba Inu",
    "Basenji",
    "Bloodhound",
    "Basset Hound",

]

output_dir = "10_dog_dataset"
images_per_breed = 10
num_steps = 30
guidance = 7.5

os.makedirs(output_dir, exist_ok=True)
for breed in dog_breeds:
    breed_dir = os.path.join(output_dir, breed.replace(" ", "_"))
    os.makedirs(breed_dir, exist_ok=True)

    for i in range(images_per_breed):
        prompt = (
            f"a high quality photo of a {breed}, "
            f"ultra realistic, cinematic lighting, 4k, sharp focus"
        )

        image = pipe(
            prompt,
            num_inference_steps=num_steps,
            guidance_scale=guidance
        ).images[0]

        filename = f"{breed.replace(' ', '_')}_{i}.png"
        image_path = os.path.join(breed_dir, filename)
        image.save(image_path)

        print(f"Saved: {image_path}")

print("Dataset generation complete.")

"""3.Generate synthetic data using the model.(for Testing Phase)"""

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

dataset_dir = "10_dog_dataset"

full_dataset = datasets.ImageFolder(
    root=dataset_dir,
    transform=transform
)

num_classes = len(full_dataset.classes)
print("Classes:", num_classes)

train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size

train_dataset, val_dataset = random_split(
    full_dataset, [train_size, val_size]
)

batch_size = 16

"""4.Provide input prompts (at least 10). also take 10 images per breed and save it"""

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = models.resnet18(pretrained=True)

# Replace final layer
model.fc = nn.Linear(model.fc.in_features, num_classes)

model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

epochs = 10

for epoch in range(epochs):
    model.train()
    running_loss = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)

    # Validation
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, preds = torch.max(outputs, 1)

            total += labels.size(0)
            correct += (preds == labels).sum().item()

    val_acc = 100 * correct / total

    print(
        f"Epoch [{epoch+1}/{epochs}] "
        f"Loss: {avg_loss:.4f} "
        f"Val Acc: {val_acc:.2f}%"
    )

torch.save(model.state_dict(), "resnet18_10_dog_breeds.pth")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

model.eval()

all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(14, 12))
sns.heatmap(
    cm,
    xticklabels=full_dataset.classes,
    yticklabels=full_dataset.classes,
    cmap="Reds",
    cbar=True
)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix – ResNet18")
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()